{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Scrollbar, Text, filedialog, messagebox\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import spacy\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tkinter import ttk\n",
    "import cv2\n",
    "import speech_recognition as sr\n",
    "from deepface import DeepFace\n",
    "from PIL import Image, ImageTk\n",
    "import pyaudio\n",
    "import pyttsx3\n",
    "import requests\n",
    "\n",
    "# Cargar el modelo desde el archivo .h5\n",
    "model = load_model('mejor_modelo.h5')\n",
    "\n",
    "API_URL = 'https://magicloops.dev/api/loop/22dfbc51-f14c-4332-a9c5-4e6e7ebc5305/run'\n",
    "\n",
    "# ==================================================\n",
    "# Configuración del Chatbot\n",
    "# ==================================================\n",
    "\n",
    "class ChatbotApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        # Frame del chatbot\n",
    "        self.chatbot_frame = ttk.LabelFrame(window, text=\"Chatbot de Alzheimer\")\n",
    "        self.chatbot_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "        # Scrollbar\n",
    "        self.scrollbar = Scrollbar(self.chatbot_frame)\n",
    "        self.chatbox = Text(self.chatbot_frame, height=15, width=60, yscrollcommand=self.scrollbar.set)\n",
    "        self.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.chatbox.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Campo de entrada y botón de envío\n",
    "        self.entry = tk.Entry(self.chatbot_frame, width=50)\n",
    "        self.entry.pack(pady=5)\n",
    "        self.send_button = tk.Button(self.chatbot_frame, text=\"Enviar\", command=self.send_message)\n",
    "        self.send_button.pack()\n",
    "\n",
    "        # Enviar mensaje al presionar Enter\n",
    "        self.entry.bind(\"<Return>\", lambda event: self.send_message())\n",
    "\n",
    "    def send_message(self):\n",
    "        user_text = self.entry.get().strip()\n",
    "        if user_text:\n",
    "            # Mostrar la pregunta en el chat\n",
    "            self.chatbox.insert(tk.END, f\"Tú: {user_text}\\n\")\n",
    "\n",
    "            # Enviar la pregunta a la API\n",
    "            response_text = self.get_response_from_api(user_text)\n",
    "\n",
    "            # Mostrar la respuesta en el chat\n",
    "            self.chatbox.insert(tk.END, f\"Bot: {response_text}\\n\\n\")\n",
    "            self.chatbox.see(tk.END)  # Desplazar al final del chat\n",
    "            self.entry.delete(0, tk.END)  # Limpiar el campo de entrada\n",
    "\n",
    "    def get_response_from_api(self, question):\n",
    "        try:\n",
    "            payload = {\"question\": question}\n",
    "            response = requests.get(API_URL, json=payload)\n",
    "\n",
    "            # Verificar si la respuesta es un JSON válido\n",
    "            try:\n",
    "                response_json = response.json()  # Intenta convertir la respuesta a JSON\n",
    "                if isinstance(response_json, dict):  # Si es un diccionario\n",
    "                    return response_json.get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "                elif isinstance(response_json, list) and len(response_json) > 0:  # Si es una lista\n",
    "                    return response_json[0].get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "                else:\n",
    "                    # Si la respuesta no es un diccionario ni una lista, devolver el texto directamente\n",
    "                    return str(response_json)\n",
    "            except ValueError:  # Si no se puede convertir a JSON\n",
    "                # Si la respuesta no es un JSON válido, devolver el texto directamente\n",
    "                return response.text\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Error: No se pudo conectar con el servidor. Detalles: {str(e)}\"\n",
    "\n",
    "# ==================================================\n",
    "# Configuración del Monitoreo\n",
    "# ==================================================\n",
    "\n",
    "# Lista de palabras esperadas (puedes modificarla según el contexto)\n",
    "palabras_esperadas = {\"hola\", \"adiós\", \"gracias\", \"por\", \"favor\", \"ayuda\", \"memoria\", \"alzheimer\"}\n",
    "\n",
    "def detectar_retraso_en_habla(tiempo_transcurrido):\n",
    "    # Umbral de tiempo para considerar un retraso (3 segundos)\n",
    "    if tiempo_transcurrido > 3.0:\n",
    "        return \"Alerta: Posible retraso en el habla detectado.\"\n",
    "    return None\n",
    "\n",
    "def detectar_palabras_incorrectas(texto):\n",
    "    palabras = texto.split()\n",
    "    palabras_incorrectas = [palabra for palabra in palabras if palabra.lower() not in palabras_esperadas]\n",
    "    \n",
    "    if palabras_incorrectas:\n",
    "        return f\"Alerta: Palabras incorrectas detectadas: {', '.join(palabras_incorrectas)}\"\n",
    "    return None\n",
    "\n",
    "def analizar_fluidez(texto):\n",
    "    palabras = texto.split()\n",
    "    if len(palabras) < 3:  # Frase muy corta\n",
    "        return \"Alerta: Frase demasiado corta. Posible dificultad para hablar.\"\n",
    "\n",
    "    # Detectar repeticiones\n",
    "    repeticiones = {}\n",
    "    for palabra in palabras:\n",
    "        repeticiones[palabra] = repeticiones.get(palabra, 0) + 1\n",
    "\n",
    "    for palabra, count in repeticiones.items():\n",
    "        if count > 3:  # Palabra repetida más de 3 veces\n",
    "            return f\"Alerta: Palabra '{palabra}' repetida {count} veces. Posible dificultad para hablar.\"\n",
    "    return None\n",
    "\n",
    "\n",
    "cap = None\n",
    "camara_encendida = False\n",
    "\n",
    "class EnhancedCameraApp:\n",
    "    def __init__(self, window, window_label, alertas_text):\n",
    "        self.window = window\n",
    "        self.window_label = window_label\n",
    "        self.alertas_text = alertas_text\n",
    "        self.video_source = 0\n",
    "        self.vid = None\n",
    "        self.is_camera_on = False\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.muted = False\n",
    "        self.tts_engine = pyttsx3.init()\n",
    "        self.voices = self.tts_engine.getProperty('voices')\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone()\n",
    "        self.is_listening = False\n",
    "        self.setup_ui()\n",
    "\n",
    "        # Vincular el cierre de la ventana para detener el hilo\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_close)\n",
    "\n",
    "    def on_close(self):\n",
    "        # Detener el reconocimiento de voz y la cámara al cerrar la ventana\n",
    "        self.stop_listening()\n",
    "        self.stop_camera()\n",
    "        self.window.destroy()\n",
    "\n",
    "    def stop_listening(self):\n",
    "        if self.is_listening:\n",
    "            self.is_listening = False\n",
    "            self.btn_listen.config(text=\"Iniciar Reconocimiento\")\n",
    "\n",
    "    def setup_ui(self):\n",
    "        control_frame = ttk.Frame(self.window)\n",
    "        control_frame.pack(pady=10)\n",
    "\n",
    "        self.btn_frame = ttk.Frame(self.window)\n",
    "        self.btn_frame.pack(pady=10)\n",
    "\n",
    "        self.btn_start = ttk.Button(self.btn_frame, text=\"Iniciar Cámara\", command=self.start_camera)\n",
    "        self.btn_start.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_stop = ttk.Button(self.btn_frame, text=\"Detener Cámara\", command=self.stop_camera, state=tk.DISABLED)\n",
    "        self.btn_stop.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_mute = ttk.Button(self.btn_frame, text=\"Silenciar\", command=self.toggle_mute)\n",
    "        self.btn_mute.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_listen = ttk.Button(self.btn_frame, text=\"Iniciar Reconocimiento\", command=self.toggle_listen)\n",
    "        self.btn_listen.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.canvas = tk.Canvas(self.window, width=640, height=480)\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def start_camera(self):\n",
    "        global cap, camara_encendida, monitoreo_activo  # Acceder a las variables globales\n",
    "        if not self.is_camera_on:\n",
    "            self.vid = cv2.VideoCapture(self.video_source)\n",
    "            if not self.vid.isOpened():\n",
    "                print(\"Error: No se pudo abrir la cámara.\")\n",
    "                return\n",
    "            self.is_camera_on = True\n",
    "            cap = self.vid  # Actualizar la variable global `cap`\n",
    "            camara_encendida = True  # Actualizar el estado de la cámara\n",
    "            activar_monitoreo(True)  # Activar el monitoreo\n",
    "            self.update_camera()\n",
    "            self.btn_start.config(state=tk.DISABLED)\n",
    "            self.btn_stop.config(state=tk.NORMAL)\n",
    "\n",
    "# Modificar la función stop_camera para desactivar el monitoreo\n",
    "    def stop_camera(self):\n",
    "        global cap, camara_encendida, monitoreo_activo  # Acceder a las variables globales\n",
    "        if self.is_camera_on:\n",
    "            self.is_camera_on = False\n",
    "            self.vid.release()\n",
    "            cap = None  # Limpiar la variable global `cap`\n",
    "            camara_encendida = False  # Actualizar el estado de la cámara\n",
    "            activar_monitoreo(False)  # Desactivar el monitoreo\n",
    "            self.canvas.delete(\"all\")\n",
    "            self.btn_start.config(state=tk.NORMAL)\n",
    "            self.btn_stop.config(state=tk.DISABLED)\n",
    "\n",
    "    def toggle_mute(self):\n",
    "        self.muted = not self.muted\n",
    "        self.btn_mute.config(text=\"Activar sonido\" if self.muted else \"Silenciar\")\n",
    "\n",
    "    def update_camera(self):\n",
    "        if self.is_camera_on:\n",
    "            ret, frame = self.vid.read()\n",
    "            if ret:\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                img = Image.fromarray(rgb)\n",
    "                imgtk = ImageTk.PhotoImage(image=img)\n",
    "                self.canvas.imgtk = imgtk\n",
    "                self.canvas.create_image(0, 0, anchor=tk.NW, image=imgtk)\n",
    "            self.window.after(30, self.update_camera)\n",
    "\n",
    "    def toggle_listen(self):\n",
    "        global monitoreo_activo\n",
    "        if not self.is_listening:\n",
    "            self.is_listening = True\n",
    "            activar_monitoreo(True)  # Activar el monitoreo\n",
    "            self.btn_listen.config(text=\"Detener Reconocimiento\")\n",
    "            threading.Thread(target=self.recognize_speech, daemon=True).start()\n",
    "        else:\n",
    "            self.is_listening = False\n",
    "            activar_monitoreo(False)  # Desactivar el monitoreo\n",
    "            self.btn_listen.config(text=\"Iniciar Reconocimiento\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def recognize_speech(self):\n",
    "        with self.microphone as source:\n",
    "        # Ajustar el umbral de ruido ambiental\n",
    "            self.recognizer.adjust_for_ambient_noise(source, duration=2)  # Ajustar durante 2 segundos\n",
    "            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Sistema listo para escuchar...\\n\")\n",
    "            self.window.after(0, self.alertas_text.see, tk.END)  # Desplazar al final del texto\n",
    "\n",
    "            while self.is_listening:\n",
    "                try:\n",
    "                    inicio = time.time()  # Tiempo inicial\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Escuchando...\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)  # Feedback visual\n",
    "                    audio = self.recognizer.listen(source, timeout=5)  # Escuchar durante 5 segundos\n",
    "                    fin = time.time()  # Tiempo final\n",
    "                    tiempo_transcurrido = fin - inicio\n",
    "\n",
    "                    texto = self.recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "                    self.window.after(0, self.display_recognized_text, texto)\n",
    "\n",
    "                # Detectar retraso en el habla\n",
    "                    alerta_retraso = detectar_retraso_en_habla(tiempo_transcurrido)\n",
    "                    if alerta_retraso:\n",
    "                        self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] {alerta_retraso}\\n\")\n",
    "                        self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "                # Detectar palabras incorrectas\n",
    "                    alerta_palabras_incorrectas = detectar_palabras_incorrectas(texto)\n",
    "                    if alerta_palabras_incorrectas:\n",
    "                        self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] {alerta_palabras_incorrectas}\\n\")\n",
    "                        self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "                # Analizar fluidez\n",
    "                    alerta_fluidez = analizar_fluidez(texto)\n",
    "                    if alerta_fluidez:\n",
    "                        self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] {alerta_fluidez}\\n\")\n",
    "                        self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "                except sr.UnknownValueError:\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] No se pudo entender el audio. Por favor, repite.\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)  # Manejo de errores\n",
    "                except sr.RequestError as e:\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Error en la solicitud al servicio de reconocimiento: {e}\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)  # Manejo de errores\n",
    "                except sr.WaitTimeoutError:\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Tiempo de espera agotado. Intenta de nuevo.\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)  # Manejo de errores\n",
    "\n",
    "    def display_recognized_text(self, text):\n",
    "        self.alertas_text.insert(tk.END, f\"[Voz Detectada] {text}\\n\")\n",
    "        self.alertas_text.see(tk.END)\n",
    "\n",
    "# Función para detectar actividad física\n",
    "prev_frame = None\n",
    "def detectar_movimiento():\n",
    "    global prev_frame, cap\n",
    "    if not camara_encendida or cap is None:\n",
    "        return False\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "    if prev_frame is None:\n",
    "        prev_frame = gray\n",
    "        return False\n",
    "    frame_delta = cv2.absdiff(prev_frame, gray)\n",
    "    prev_frame = gray\n",
    "    movimiento = np.sum(frame_delta) > 500000  # Umbral de cambio en píxeles\n",
    "    return movimiento\n",
    "\n",
    "# Función para detectar emociones\n",
    "emociones_traduccion = {\n",
    "    \"angry\": \"Enojo\",\n",
    "    \"disgust\": \"Disgusto\",\n",
    "    \"fear\": \"Miedo\",\n",
    "    \"happy\": \"Feliz\",\n",
    "    \"sad\": \"Tristeza\",\n",
    "    \"surprise\": \"Sorpresa\",\n",
    "    \"neutral\": \"Neutral\"\n",
    "}\n",
    "\n",
    "def detectar_emocion():\n",
    "    global cap\n",
    "    if not camara_encendida or cap is None:\n",
    "        return \"No se detecta cámara\"\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return \"No se pudo capturar imagen\"\n",
    "    try:\n",
    "        # Detectar rostros en la imagen usando DeepFace.extract_faces\n",
    "        rostros = DeepFace.extract_faces(frame, detector_backend='mtcnn')\n",
    "\n",
    "        if len(rostros) > 0:\n",
    "            # Tomar el primer rostro detectado\n",
    "            rostro = rostros[0]['face']\n",
    "            \n",
    "            # Analizar la emoción en el rostro detectado\n",
    "            resultados = DeepFace.analyze(rostro, actions=['emotion'], enforce_detection=False)\n",
    "            emocion = resultados[0]['dominant_emotion']\n",
    "            \n",
    "            # Traducir la emoción al español\n",
    "            emocion_espanol = emociones_traduccion.get(emocion, \"Emoción desconocida\")\n",
    "            return f\"Emoción detectada: {emocion_espanol}\"\n",
    "        else:\n",
    "            return \"No se detectó un rostro en la imagen\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al detectar emoción, es posible que la cámara no esté detectando correctamente al usuario, por favor, asegurese de que la cámara capte totalmente el rostro de la persona\"\n",
    "\n",
    "# Configuración de reconocimiento de voz\n",
    "recognizer = sr.Recognizer()\n",
    "def detectar_cambios_habla():\n",
    "    try:\n",
    "        with sr.Microphone() as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)\n",
    "                texto = recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "                return texto\n",
    "            except sr.WaitTimeoutError:\n",
    "                return \"Alerta: No se detectó sonido en el tiempo esperado.\"\n",
    "            except sr.UnknownValueError:\n",
    "                return \"Alerta: No se detectó habla.\"\n",
    "            except sr.RequestError:\n",
    "                return \"Alerta: Error en reconocimiento.\"\n",
    "    except OSError:\n",
    "        return \"Alerta: No se detecta micrófono, no se puede realizar monitoreo del habla.\"\n",
    "\n",
    "# Variable global para controlar el estado del monitoreo\n",
    "monitoreo_activo = False\n",
    "app_running = True\n",
    "\n",
    "# Función para activar/desactivar el monitoreo\n",
    "def activar_monitoreo(estado):\n",
    "    global monitoreo_activo\n",
    "    monitoreo_activo = estado\n",
    "    \n",
    "\n",
    "# Monitoreo en tiempo real\n",
    "def monitorear_paciente():\n",
    "    global app_running\n",
    "    while app_running:  # Solo monitorear si la aplicación está en ejecución\n",
    "        if monitoreo_activo:\n",
    "            actividad_fisica = detectar_movimiento()\n",
    "            habla = detectar_cambios_habla()\n",
    "            emocion = detectar_emocion()\n",
    "            alertas = []\n",
    "            \n",
    "            if not actividad_fisica:\n",
    "                alertas.append(\"Alerta: Baja actividad física detectada.\")\n",
    "            if \"No se detectó habla\" in habla:\n",
    "                alertas.append(\"Alerta: Cambios en el habla detectados.\")\n",
    "            \n",
    "            alertas.append(emocion)\n",
    "            \n",
    "            for alerta in alertas:\n",
    "                alertas_text.insert(tk.END, f\"[{datetime.now()}] {alerta}\\n\")\n",
    "                alertas_text.see(tk.END)\n",
    "        \n",
    "        time.sleep(8)\n",
    "\n",
    "def on_close():\n",
    "    global app_running\n",
    "    app_running = False  # Detener el hilo de monitoreo\n",
    "    camera_app.on_close()  # Detener el reconocimiento de voz y la cámara\n",
    "    ventana.destroy()\n",
    "\n",
    "\n",
    "\n",
    "# Función para predecir\n",
    "def seleccionar_imagen():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    title=\"Selecciona una imagen\",\n",
    "    filetypes=((\"Archivos de imagen\", \".jpg *.jpeg *.png\"), (\"Todos los archivos\", \".*\"))\n",
    "    return file_path\n",
    "\n",
    "def predecir_imagen(file_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(file_path, target_size=(150, 150))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediccion = model.predict(img)\n",
    "    clase_predicha = np.argmax(prediccion, axis=1)\n",
    "    return clase_predicha[0]\n",
    "\n",
    "def predecir_y_mostrar():\n",
    "    file_path = seleccionar_imagen()\n",
    "    if file_path:\n",
    "        clase_predicha = predecir_imagen(file_path)\n",
    "        if clase_predicha == 0:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que se encuentra en la etapa leve del alzheimer.\\n Se sugiere que opte por participar en actividades cognitivas y físicas para estimular el cerebro.\\n Consultar regularmente in médico especializado en neurología.\\n  Se observa una disminución en el volumen del tejido cerebral, especialmente en áreas como el hipocampo (crucial para la memoria) y la corteza cerebral. Esta atrofia es un indicador clave de la enfermedad de Alzheimer\"\n",
    "        elif clase_predicha == 1:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que se encuentra en la etapa avanzada del alzheimer. \\n Se observa una atrofia cerebral más pronunciada. \\n Los ventrículos están más agrandados y la corteza cerebral muestra una mayor pérdida de volumen.\\n Se recomienda evaluaciones médicas de forma frecuente\"\n",
    "        elif clase_predicha == 2:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que no posee alzheimer. \\n  En general, la estructura cerebral parece estar dentro de los límites normales para la edad de la persona. \\n No se observa una atrofia cerebral significativa. El volumen del tejido cerebral parece conservado.\"\n",
    "        elif clase_predicha == 3:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que puede tener indicios de padecer alzheimer. \\n Se observa que el patrón de atrofia es consistente con la enfermedad de Alzheimer en una etapa muy temprana. Por lo tanto, existe una posibilidad de que esta persona padezca de alzheimer \"\n",
    "        else:\n",
    "            resultado = \"La imagen proporcionada es incorrecta.\"\n",
    "        resultado_text.delete(1.0, tk.END)  # Limpiar el texto anterior\n",
    "        resultado_text.insert(tk.END, resultado)\n",
    "\n",
    "# ==================================================\n",
    "# Interfaz gráfica\n",
    "# ==================================================\n",
    "\n",
    "ventana = tk.Tk()\n",
    "ventana.title(\"Memory Vision\")\n",
    "ventana.geometry(\"800x600\")\n",
    "\n",
    "notebook = ttk.Notebook(ventana)\n",
    "notebook.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "### Pestaña Monitoreo ###\n",
    "monitoreo_frame = ttk.Frame(notebook)\n",
    "notebook.add(monitoreo_frame, text=\"Monitoreo\")\n",
    "\n",
    "alertas_frame = tk.LabelFrame(monitoreo_frame, text=\"Alertas\")\n",
    "alertas_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "alertas_text = tk.Text(alertas_frame, height=15, width=80)\n",
    "alertas_text.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "camara_frame = ttk.Frame(monitoreo_frame)\n",
    "camara_frame.pack(pady=10)\n",
    "\n",
    "camara_label = tk.Label(camara_frame)\n",
    "camara_label.pack()\n",
    "\n",
    "camera_app = EnhancedCameraApp(ventana, camara_label, alertas_text)\n",
    "\n",
    "boton_camara = tk.Button(monitoreo_frame, text=\"Encender Cámara\", command=camera_app.start_camera)\n",
    "boton_camara.pack()\n",
    "\n",
    "### Pestaña Chatbot ###\n",
    "chatbot_frame = ttk.Frame(notebook)\n",
    "notebook.add(chatbot_frame, text=\"Chatbot\")\n",
    "\n",
    "chatbot_system = ChatbotApp(chatbot_frame)\n",
    "\n",
    "### Pestaña Predicción ###\n",
    "prediccion_frame = ttk.Frame(notebook)\n",
    "notebook.add(prediccion_frame, text=\"Predicción\")\n",
    "\n",
    "pred_frame = tk.LabelFrame(prediccion_frame, text=\"Predicción de la imagen\")\n",
    "pred_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "predecir_button = tk.Button(pred_frame, text=\"Seleccionar Imagen y Predecir\", command=predecir_y_mostrar)\n",
    "predecir_button.pack(pady=5)\n",
    "\n",
    "resultado_text = tk.Text(pred_frame, height=5, width=80)\n",
    "resultado_text.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Iniciar monitoreo en segundo plano\n",
    "monitoreo_thread = threading.Thread(target=monitorear_paciente, daemon=True)\n",
    "monitoreo_thread.start()\n",
    "\n",
    "ventana.mainloop()\n",
    "ventana.protocol(\"WM_DELETE_WINDOW\", on_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
