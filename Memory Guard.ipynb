{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Scrollbar, Text, filedialog, messagebox\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import spacy\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tkinter import ttk\n",
    "import cv2\n",
    "import speech_recognition as sr\n",
    "from deepface import DeepFace\n",
    "from PIL import Image, ImageTk\n",
    "import pyaudio\n",
    "import pyttsx3\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el modelo desde el archivo .h5\n",
    "model = load_model('mejor_modelo.h5')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "API_URL = 'https://magicloops.dev/api/loop/22dfbc51-f14c-4332-a9c5-4e6e7ebc5305/run'\n",
    "\n",
    "# ==================================================\n",
    "# Configuración del Chatbot\n",
    "# ==================================================\n",
    "\n",
    "class ChatbotApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        # Frame del chatbot\n",
    "        self.chatbot_frame = ttk.LabelFrame(window, text=\"Chatbot de Alzheimer\")\n",
    "        self.chatbot_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "        # Scrollbar\n",
    "        self.scrollbar = Scrollbar(self.chatbot_frame)\n",
    "        self.chatbox = Text(self.chatbot_frame, height=15, width=60, yscrollcommand=self.scrollbar.set)\n",
    "        self.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.chatbox.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Campo de entrada y botón de envío\n",
    "        self.entry = tk.Entry(self.chatbot_frame, width=50)\n",
    "        self.entry.pack(pady=5)\n",
    "        self.send_button = tk.Button(self.chatbot_frame, text=\"Enviar\", command=self.send_message)\n",
    "        self.send_button.pack()\n",
    "\n",
    "        # Enviar mensaje al presionar Enter\n",
    "        self.entry.bind(\"<Return>\", lambda event: self.send_message())\n",
    "\n",
    "    def send_message(self):\n",
    "        user_text = self.entry.get().strip()\n",
    "        if user_text:\n",
    "            # Mostrar la pregunta en el chat\n",
    "            self.chatbox.insert(tk.END, f\"Tú: {user_text}\\n\")\n",
    "\n",
    "            # Enviar la pregunta a la API\n",
    "            response_text = self.get_response_from_api(user_text)\n",
    "\n",
    "            # Mostrar la respuesta en el chat\n",
    "            self.chatbox.insert(tk.END, f\"Bot: {response_text}\\n\\n\")\n",
    "            self.chatbox.see(tk.END)  # Desplazar al final del chat\n",
    "            self.entry.delete(0, tk.END)  # Limpiar el campo de entrada\n",
    "\n",
    "    def get_response_from_api(self, question):\n",
    "        try:\n",
    "            payload = {\"question\": question}\n",
    "            response = requests.get(API_URL, json=payload)\n",
    "\n",
    "            # Verificar si la respuesta es un JSON válido\n",
    "            try:\n",
    "                response_json = response.json()  # Intenta convertir la respuesta a JSON\n",
    "                if isinstance(response_json, dict):  # Si es un diccionario\n",
    "                    return response_json.get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "                elif isinstance(response_json, list) and len(response_json) > 0:  # Si es una lista\n",
    "                    return response_json[0].get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "                else:\n",
    "                    # Si la respuesta no es un diccionario ni una lista, devolver el texto directamente\n",
    "                    return str(response_json)\n",
    "            except ValueError:  # Si no se puede convertir a JSON\n",
    "                # Si la respuesta no es un JSON válido, devolver el texto directamente\n",
    "                return response.text\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Error: No se pudo conectar con el servidor. Detalles: {str(e)}\"\n",
    "\n",
    "# ==================================================\n",
    "# Configuración del Monitoreo\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "def calcular_velocidad_habla(texto, tiempo_transcurrido):\n",
    "    palabras = texto.split()\n",
    "    if tiempo_transcurrido > 0:\n",
    "        velocidad = len(palabras) / tiempo_transcurrido  # Palabras por segundo\n",
    "        return velocidad\n",
    "    return 0\n",
    "\n",
    "\n",
    "def detectar_pausas_largas(audio, umbral_silencio=0.02, duracion_minima_pausa=5.0):\n",
    "    # Convertir el audio a un array de numpy\n",
    "    audio_data = np.frombuffer(audio.frame_data, dtype=np.int16)\n",
    "    \n",
    "    # Calcular la amplitud máxima del audio\n",
    "    amplitud_maxima = np.max(np.abs(audio_data))\n",
    "    \n",
    "    # Definir el umbral de silencio en función de la amplitud máxima\n",
    "    umbral_amplitud = umbral_silencio * amplitud_maxima\n",
    "    \n",
    "    # Detectar segmentos de silencio\n",
    "    silencio = np.where(np.abs(audio_data) < umbral_amplitud)[0]\n",
    "    \n",
    "    # Si no hay silencios, retornar 0\n",
    "    if len(silencio) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calcular la duración de los silencios\n",
    "    duracion_silencio = len(silencio) / audio.sample_rate\n",
    "    \n",
    "    # Solo considerar pausas largas\n",
    "    if duracion_silencio >= duracion_minima_pausa:\n",
    "        return duracion_silencio\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "\n",
    "def detectar_retraso_en_habla(texto, tiempo_transcurrido, audio):\n",
    "    if not texto:  # Si no hay texto, no hay alerta\n",
    "        return None\n",
    "\n",
    "    # Calcular velocidad del habla\n",
    "    velocidad = calcular_velocidad_habla(texto, tiempo_transcurrido)\n",
    "    \n",
    "    # Detectar pausas largas\n",
    "    duracion_pausas = detectar_pausas_largas(audio)\n",
    "    \n",
    "    # Umbrales ajustados\n",
    "    umbral_velocidad = 1.0  # Menos de 1 palabra por segundo\n",
    "    umbral_pausas = 6.0     # Pausas de más de 2 segundos\n",
    "    \n",
    "    alertas = []\n",
    "    if velocidad < umbral_velocidad:\n",
    "        alertas.append(f\"Velocidad del habla muy lenta ({velocidad:.1f} palabras/segundo).\")\n",
    "    if duracion_pausas > umbral_pausas:\n",
    "        alertas.append(f\"Pausas largas detectadas ({duracion_pausas:.1f} segundos).\")\n",
    "    \n",
    "    if alertas:\n",
    "        return \"Alerta: Posible retraso en el habla. \" + \" \".join(alertas)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def analizar_fluidez(texto, tiempo_transcurrido):\n",
    "    palabras = texto.split()\n",
    "    if len(palabras) < 3:  # Frase muy corta\n",
    "        return \"Alerta: Frase demasiado corta. Posible dificultad para hablar.\"\n",
    "\n",
    "    # Detectar repeticiones\n",
    "    repeticiones = {}\n",
    "    for palabra in palabras:\n",
    "        repeticiones[palabra] = repeticiones.get(palabra, 0) + 1\n",
    "\n",
    "    for palabra, count in repeticiones.items():\n",
    "        if count > 3:  # Palabra repetida más de 3 veces\n",
    "            return f\"Alerta: Palabra '{palabra}' repetida {count} veces. Posible dificultad para hablar.\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def analizar_coherencia(texto):\n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    # Verificar si la oración tiene un verbo\n",
    "    tiene_verbo = any(token.pos_ == \"VERB\" for token in doc)\n",
    "    \n",
    "    # Verificar si la oración tiene un sujeto\n",
    "    tiene_sujeto = any(token.dep_ == \"nsubj\" for token in doc)\n",
    "    \n",
    "    # Verificar si la oración tiene un objeto (opcional)\n",
    "    tiene_objeto = any(token.dep_ in (\"obj\", \"dobj\") for token in doc)\n",
    "    \n",
    "    # Verificar si la oración es demasiado corta (menos de 3 palabras)\n",
    "    es_muy_corta = len(doc) < 3\n",
    "    \n",
    "    # Verificar si la oración tiene una estructura básica (sujeto + verbo)\n",
    "    es_coherente = tiene_verbo and tiene_sujeto and not es_muy_corta\n",
    "    \n",
    "    if not es_coherente:\n",
    "        return \"Alerta: La frase no parece tener una estructura coherente.\"\n",
    "    \n",
    "    # Verificar si las relaciones sintácticas son coherentes\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\" and token.pos_ != \"VERB\":\n",
    "            return \"Alerta: La frase no tiene un verbo principal.\"\n",
    "    \n",
    "    return \"La frase parece coherente.\"\n",
    "\n",
    "\n",
    "cap = None\n",
    "camara_encendida = False\n",
    "\n",
    "class EnhancedCameraApp:\n",
    "    def __init__(self, window, window_label, alertas_text):\n",
    "        self.window = window\n",
    "        self.window_label = window_label\n",
    "        self.alertas_text = alertas_text\n",
    "        self.video_source = 0\n",
    "        self.vid = None\n",
    "        self.is_camera_on = False\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.muted = False\n",
    "        self.tts_engine = pyttsx3.init()\n",
    "        self.voices = self.tts_engine.getProperty('voices')\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone()\n",
    "        self.is_listening = False\n",
    "        self.setup_ui()\n",
    "\n",
    "        # Vincular el cierre de la ventana para detener el hilo\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_close)\n",
    "\n",
    "    def on_close(self):\n",
    "        # Detener el reconocimiento de voz y la cámara al cerrar la ventana\n",
    "        self.stop_listening()\n",
    "        self.stop_camera()\n",
    "        self.window.destroy()\n",
    "\n",
    "    def stop_listening(self):\n",
    "        if self.is_listening:\n",
    "            self.is_listening = False\n",
    "            self.btn_listen.config(text=\"Iniciar Reconocimiento\")\n",
    "\n",
    "    def setup_ui(self):\n",
    "        control_frame = ttk.Frame(self.window)\n",
    "        control_frame.pack(pady=10)\n",
    "\n",
    "        self.btn_frame = ttk.Frame(self.window)\n",
    "        self.btn_frame.pack(pady=10)\n",
    "\n",
    "        self.btn_start = ttk.Button(self.btn_frame, text=\"Iniciar Cámara\", command=self.start_camera)\n",
    "        self.btn_start.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_stop = ttk.Button(self.btn_frame, text=\"Detener Cámara\", command=self.stop_camera, state=tk.DISABLED)\n",
    "        self.btn_stop.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_mute = ttk.Button(self.btn_frame, text=\"Silenciar\", command=self.toggle_mute)\n",
    "        self.btn_mute.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_listen = ttk.Button(self.btn_frame, text=\"Iniciar Reconocimiento\", command=self.toggle_listen)\n",
    "        self.btn_listen.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.canvas = tk.Canvas(self.window, width=640, height=480)\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def start_camera(self):\n",
    "        global cap, camara_encendida, monitoreo_activo  # Acceder a las variables globales\n",
    "        if not self.is_camera_on:\n",
    "            self.vid = cv2.VideoCapture(self.video_source)\n",
    "            if not self.vid.isOpened():\n",
    "                print(\"Error: No se pudo abrir la cámara.\")\n",
    "                return\n",
    "            self.is_camera_on = True\n",
    "            cap = self.vid  # Actualizar la variable global `cap`\n",
    "            camara_encendida = True  # Actualizar el estado de la cámara\n",
    "            activar_monitoreo(True)  # Activar el monitoreo\n",
    "            self.update_camera()\n",
    "            self.btn_start.config(state=tk.DISABLED)\n",
    "            self.btn_stop.config(state=tk.NORMAL)\n",
    "\n",
    "# Modificar la función stop_camera para desactivar el monitoreo\n",
    "    def stop_camera(self):\n",
    "        global cap, camara_encendida, monitoreo_activo  # Acceder a las variables globales\n",
    "        if self.is_camera_on:\n",
    "            self.is_camera_on = False\n",
    "            self.vid.release()\n",
    "            cap = None  # Limpiar la variable global `cap`\n",
    "            camara_encendida = False  # Actualizar el estado de la cámara\n",
    "            activar_monitoreo(False)  # Desactivar el monitoreo\n",
    "            self.canvas.delete(\"all\")\n",
    "            self.btn_start.config(state=tk.NORMAL)\n",
    "            self.btn_stop.config(state=tk.DISABLED)\n",
    "\n",
    "    def toggle_mute(self):\n",
    "        self.muted = not self.muted\n",
    "        self.btn_mute.config(text=\"Activar sonido\" if self.muted else \"Silenciar\")\n",
    "\n",
    "    def update_camera(self):\n",
    "        if self.is_camera_on:\n",
    "            ret, frame = self.vid.read()\n",
    "            if ret:\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                img = Image.fromarray(rgb)\n",
    "                imgtk = ImageTk.PhotoImage(image=img)\n",
    "                self.canvas.imgtk = imgtk\n",
    "                self.canvas.create_image(0, 0, anchor=tk.NW, image=imgtk)\n",
    "            self.window.after(30, self.update_camera)\n",
    "\n",
    "    def toggle_listen(self):\n",
    "        global monitoreo_activo\n",
    "        if not self.is_listening:\n",
    "            self.is_listening = True\n",
    "            activar_monitoreo(True)  # Activar el monitoreo\n",
    "            self.btn_listen.config(text=\"Detener Reconocimiento\")\n",
    "            threading.Thread(target=self.recognize_speech, daemon=True).start()\n",
    "        else:\n",
    "            self.is_listening = False\n",
    "            activar_monitoreo(False)  # Desactivar el monitoreo\n",
    "            self.btn_listen.config(text=\"Iniciar Reconocimiento\")\n",
    "    def recognize_speech(self):\n",
    "        with self.microphone as source:\n",
    "        # Ajustar el umbral de ruido ambiental\n",
    "            self.recognizer.adjust_for_ambient_noise(source, duration=2)\n",
    "            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Sistema listo para escuchar...\\n\")\n",
    "            self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "            while self.is_listening:\n",
    "                try:\n",
    "                    inicio = time.time()\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Escuchando...\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "                    # Escuchar el audio desde el micrófono\n",
    "                    audio = self.recognizer.listen(source, timeout=5)\n",
    "                    fin = time.time()\n",
    "                    tiempo_transcurrido = fin - inicio\n",
    "                    texto = self.recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "\n",
    "                # Transcribir el audio usando Google Web Speech API\n",
    "                    texto = self.recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "                    self.window.after(0, self.display_recognized_text, texto)\n",
    "\n",
    "                    alerta_retraso = detectar_retraso_en_habla(texto, tiempo_transcurrido, audio)\n",
    "                    if alerta_retraso:\n",
    "                        self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] {alerta_retraso}\\n\")\n",
    "                        self.alertas_text.see(tk.END)\n",
    "\n",
    "                # Analizar fluidez\n",
    "                    alerta_fluidez = analizar_fluidez(texto, tiempo_transcurrido)\n",
    "                    if alerta_fluidez:\n",
    "                        self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] {alerta_fluidez}\\n\")\n",
    "                        self.alertas_text.see(tk.END)\n",
    "\n",
    "                except sr.UnknownValueError:\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] No se pudo entender el audio. Por favor, repite.\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)\n",
    "                except sr.RequestError as e:\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Error en la solicitud al servicio de reconocimiento: {e}\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)\n",
    "                except sr.WaitTimeoutError:\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Tiempo de espera agotado. Intenta de nuevo.\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "\n",
    "    def display_recognized_text(self, text):\n",
    "        self.alertas_text.insert(tk.END, f\"[Voz Detectada] {text}\\n\")\n",
    "        self.alertas_text.see(tk.END)\n",
    "\n",
    "# Función para detectar actividad física\n",
    "prev_frame = None\n",
    "def detectar_movimiento():\n",
    "    global prev_frame, cap\n",
    "    if not camara_encendida or cap is None:\n",
    "        return False\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return False\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "    if prev_frame is None:\n",
    "        prev_frame = gray\n",
    "        return False\n",
    "    frame_delta = cv2.absdiff(prev_frame, gray)\n",
    "    prev_frame = gray\n",
    "    movimiento = np.sum(frame_delta) > 500000  # Umbral de cambio en píxeles\n",
    "    return movimiento\n",
    "\n",
    "# Función para detectar emociones\n",
    "emociones_traduccion = {\n",
    "    \"angry\": \"Enojo\",\n",
    "    \"disgust\": \"Disgusto\",\n",
    "    \"fear\": \"Miedo\",\n",
    "    \"happy\": \"Feliz\",\n",
    "    \"sad\": \"Tristeza\",\n",
    "    \"surprise\": \"Sorpresa\",\n",
    "    \"neutral\": \"Neutral\"\n",
    "}\n",
    "\n",
    "def detectar_emocion():\n",
    "    global cap\n",
    "    if not camara_encendida or cap is None:\n",
    "        return \"No se detecta cámara\"\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return \"No se pudo capturar imagen\"\n",
    "    try:\n",
    "        # Detectar rostros en la imagen usando DeepFace.extract_faces\n",
    "        rostros = DeepFace.extract_faces(frame, detector_backend='mtcnn')\n",
    "\n",
    "        if len(rostros) > 0:\n",
    "            # Tomar el primer rostro detectado\n",
    "            rostro = rostros[0]['face']\n",
    "            \n",
    "            # Analizar la emoción en el rostro detectado\n",
    "            resultados = DeepFace.analyze(rostro, actions=['emotion'], enforce_detection=False)\n",
    "            emocion = resultados[0]['dominant_emotion']\n",
    "            \n",
    "            # Traducir la emoción al español\n",
    "            emocion_espanol = emociones_traduccion.get(emocion, \"Emoción desconocida\")\n",
    "            return f\"Emoción detectada: {emocion_espanol}\"\n",
    "        else:\n",
    "            return \"No se detectó un rostro en la imagen\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al detectar emoción, es posible que la cámara no esté detectando correctamente al usuario, por favor, asegurese de que la cámara capte totalmente el rostro de la persona\"\n",
    "\n",
    "# Configuración de reconocimiento de voz\n",
    "recognizer = sr.Recognizer()\n",
    "def detectar_cambios_habla():\n",
    "    try:\n",
    "        with sr.Microphone() as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)\n",
    "                texto = recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "                \n",
    "                # Analizar fluidez y coherencia del habla\n",
    "                alerta_fluidez = analizar_fluidez(texto)\n",
    "                if alerta_fluidez:\n",
    "                    return alerta_fluidez\n",
    "                \n",
    "                # Analizar coherencia semántica (opcional)\n",
    "                alerta_coherencia = analizar_coherencia(texto)\n",
    "                if alerta_coherencia:\n",
    "                    return alerta_coherencia\n",
    "                \n",
    "                return texto\n",
    "            except sr.WaitTimeoutError:\n",
    "                return \"Alerta: No se detectó sonido en el tiempo esperado.\"\n",
    "            except sr.UnknownValueError:\n",
    "                return \"Alerta: No se detectó habla.\"\n",
    "            except sr.RequestError:\n",
    "                return \"Alerta: Error en reconocimiento.\"\n",
    "    except OSError:\n",
    "        return \"Alerta: No se detecta micrófono, no se puede realizar monitoreo del habla.\"\n",
    "\n",
    "# Variable global para controlar el estado del monitoreo\n",
    "monitoreo_activo = False\n",
    "app_running = True\n",
    "\n",
    "# Función para activar/desactivar el monitoreo\n",
    "def activar_monitoreo(estado):\n",
    "    global monitoreo_activo\n",
    "    monitoreo_activo = estado\n",
    "    \n",
    "\n",
    "# Monitoreo en tiempo real\n",
    "def monitorear_paciente():\n",
    "    global app_running, camara_encendida\n",
    "    while app_running:  # Solo monitorear si la aplicación está en ejecución\n",
    "        if monitoreo_activo and camara_encendida:  # Solo monitorear si la cámara está encendida\n",
    "            actividad_fisica = detectar_movimiento()\n",
    "            habla = detectar_cambios_habla()\n",
    "            emocion = detectar_emocion()\n",
    "            alertas = []\n",
    "            \n",
    "            if not actividad_fisica:\n",
    "                alertas.append(\"Alerta: Baja actividad física detectada.\")\n",
    "            if \"No se detectó habla\" in habla:\n",
    "                alertas.append(\"Alerta: Cambios en el habla detectados.\")\n",
    "            \n",
    "            alertas.append(emocion)\n",
    "            \n",
    "            for alerta in alertas:\n",
    "                alertas_text.insert(tk.END, f\"[{datetime.now()}] {alerta}\\n\")\n",
    "                alertas_text.see(tk.END)\n",
    "        \n",
    "        time.sleep(120)\n",
    "\n",
    "def on_close():\n",
    "    global app_running\n",
    "    app_running = False  # Detener el hilo de monitoreo\n",
    "    camera_app.on_close()  # Detener el reconocimiento de voz y la cámara\n",
    "    ventana.destroy()\n",
    "\n",
    "\n",
    "\n",
    "# Función para predecir\n",
    "def seleccionar_imagen():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    title=\"Selecciona una imagen\",\n",
    "    filetypes=((\"Archivos de imagen\", \".jpg *.jpeg *.png\"), (\"Todos los archivos\", \".*\"))\n",
    "    return file_path\n",
    "\n",
    "def predecir_imagen(file_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(file_path, target_size=(150, 150))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediccion = model.predict(img)\n",
    "    clase_predicha = np.argmax(prediccion, axis=1)\n",
    "    return clase_predicha[0]\n",
    "\n",
    "def predecir_y_mostrar():\n",
    "    file_path = seleccionar_imagen()\n",
    "    if file_path:\n",
    "        clase_predicha = predecir_imagen(file_path)\n",
    "        if clase_predicha == 0:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que se encuentra en la etapa leve del alzheimer.\\n Se sugiere que opte por participar en actividades cognitivas y físicas para estimular el cerebro.\\n Consultar regularmente in médico especializado en neurología.\\n  Se observa una disminución en el volumen del tejido cerebral, especialmente en áreas como el hipocampo (crucial para la memoria) y la corteza cerebral. Esta atrofia es un indicador clave de la enfermedad de Alzheimer\"\n",
    "        elif clase_predicha == 1:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que se encuentra en la etapa avanzada del alzheimer. \\n Se observa una atrofia cerebral más pronunciada. \\n Los ventrículos están más agrandados y la corteza cerebral muestra una mayor pérdida de volumen.\\n Se recomienda evaluaciones médicas de forma frecuente\"\n",
    "        elif clase_predicha == 2:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que no posee alzheimer. \\n  En general, la estructura cerebral parece estar dentro de los límites normales para la edad de la persona. \\n No se observa una atrofia cerebral significativa. El volumen del tejido cerebral parece conservado.\"\n",
    "        elif clase_predicha == 3:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que puede tener indicios de padecer alzheimer. \\n Se observa que el patrón de atrofia es consistente con la enfermedad de Alzheimer en una etapa muy temprana. Por lo tanto, existe una posibilidad de que esta persona padezca de alzheimer \"\n",
    "        else:\n",
    "            resultado = \"La imagen proporcionada es incorrecta.\"\n",
    "        resultado_text.delete(1.0, tk.END)  # Limpiar el texto anterior\n",
    "        resultado_text.insert(tk.END, resultado)\n",
    "\n",
    "# ==================================================\n",
    "# Interfaz gráfica\n",
    "# ==================================================\n",
    "\n",
    "ventana = tk.Tk()\n",
    "ventana.title(\"Memory Vision\")\n",
    "ventana.geometry(\"800x600\")\n",
    "\n",
    "notebook = ttk.Notebook(ventana)\n",
    "notebook.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "### Pestaña Monitoreo ###\n",
    "monitoreo_frame = ttk.Frame(notebook)\n",
    "notebook.add(monitoreo_frame, text=\"Monitoreo\")\n",
    "\n",
    "alertas_frame = tk.LabelFrame(monitoreo_frame, text=\"Alertas\")\n",
    "alertas_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "alertas_text = tk.Text(alertas_frame, height=15, width=80)\n",
    "alertas_text.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "camara_frame = ttk.Frame(monitoreo_frame)\n",
    "camara_frame.pack(pady=10)\n",
    "\n",
    "camara_label = tk.Label(camara_frame)\n",
    "camara_label.pack()\n",
    "\n",
    "camera_app = EnhancedCameraApp(ventana, camara_label, alertas_text)\n",
    "\n",
    "### Pestaña Chatbot ###\n",
    "chatbot_frame = ttk.Frame(notebook)\n",
    "notebook.add(chatbot_frame, text=\"Chatbot\")\n",
    "\n",
    "chatbot_system = ChatbotApp(chatbot_frame)\n",
    "\n",
    "### Pestaña Predicción ###\n",
    "prediccion_frame = ttk.Frame(notebook)\n",
    "notebook.add(prediccion_frame, text=\"Predicción\")\n",
    "\n",
    "pred_frame = tk.LabelFrame(prediccion_frame, text=\"Predicción de la imagen\")\n",
    "pred_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "predecir_button = tk.Button(pred_frame, text=\"Seleccionar Imagen y Predecir\", command=predecir_y_mostrar)\n",
    "predecir_button.pack(pady=5)\n",
    "\n",
    "resultado_text = tk.Text(pred_frame, height=5, width=80)\n",
    "resultado_text.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Iniciar monitoreo en segundo plano\n",
    "monitoreo_thread = threading.Thread(target=monitorear_paciente, daemon=True)\n",
    "monitoreo_thread.start()\n",
    "\n",
    "ventana.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Voz Detectada] no comprendo tengo el reconocimiento rápido o no hay reconocimiento rápido\n",
    "[2025-03-11 22:49:20.615158] Alerta: Posible retraso en el habla. Pausas largas detectadas (5.1 segundos).\n",
    "[2025-03-11 22:49:20.618639] Escuchando...\n",
    "[2025-03-11 22:49:24.982447] No se pudo entender el audio. Por favor, repite.\n",
    "[2025-03-11 22:49:24.985596] Escuchando...\n",
    "[Voz Detectada] puedes captar de forma inmediata todo lo que yo transmito en un mensaje\n",
    "[2025-03-11 22:49:32.824415] Alerta: Posible retraso en el habla. Pausas largas detectadas (5.1 segundos).\n",
    "[2025-03-11 22:49:32.827426] Escuchando...\n",
    "[Voz Detectada] Pero por qué me detecta el retraso en el habla si yo no tengo pausas largas\n",
    "[2025-03-11 22:49:43.227305] Alerta: Posible retraso en el habla. Pausas largas detectadas (4.3 segundos).\n",
    "[2025-03-11 22:49:43.228305] Escuchando..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\es_core_news_sm\n"
     ]
    }
   ],
   "source": [
    "print(spacy.util.get_package_path(\"es_core_news_sm\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
