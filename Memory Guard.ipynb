{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Predicciones: [[0.16659023 0.14677545 0.16814278 0.15393297 0.36455855]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step\n",
      "Predicciones: [[0.16658373 0.14676848 0.16813761 0.15393624 0.36457393]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "Predicciones: [[0.16658668 0.14677243 0.16813809 0.15393773 0.36456504]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step\n",
      "Predicciones: [[0.16664326 0.14672238 0.16809472 0.1538975  0.3646421 ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "Predicciones: [[0.16664585 0.14673267 0.16810194 0.15390407 0.36461547]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Predicciones: [[0.16665804 0.14672616 0.16809395 0.15390398 0.3646178 ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Predicciones: [[0.16664042 0.14672059 0.16809553 0.15389699 0.3646465 ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Predicciones: [[0.16664805 0.14672606 0.16809945 0.15390144 0.364625  ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "Predicciones: [[0.16665874 0.14672919 0.16809174 0.1539025  0.3646178 ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "Predicciones: [[0.1666478  0.14672521 0.16809294 0.15389866 0.3646354 ]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import Scrollbar, Text, filedialog, messagebox\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import spacy\n",
    "import subprocess\n",
    "import threading\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tkinter import ttk\n",
    "import cv2\n",
    "import speech_recognition as sr\n",
    "from deepface import DeepFace\n",
    "from PIL import Image, ImageTk\n",
    "import pyaudio\n",
    "import pyttsx3\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov4.weights\", \"yolov4.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "\n",
    "\n",
    "# Cargar nombres de clases\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el modelo desde el archivo .h5\n",
    "model = load_model('mejor_modelo.h5')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Cargar el modelo de emociones\n",
    "emociones_model = load_model('emotion_model.h5')\n",
    "emociones_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "API_URL = 'https://magicloops.dev/api/loop/22dfbc51-f14c-4332-a9c5-4e6e7ebc5305/run'\n",
    "\n",
    "### Paleta de colores ###\n",
    "\n",
    "COLOR_FONDO_PRINCIPAL = \"#E3F2FD\"  # Azul claro\n",
    "COLOR_FONDO_SECUNDARIO = \"#90CAF9\"  # Azul más oscuro\n",
    "COLOR_TEXTO_PRINCIPAL = \"#212121\"  # Gris oscuro\n",
    "COLOR_BOTONES = \"#42A5F5\"  # Azul medio\n",
    "COLOR_BOTONES_HOVER = \"#64B5F6\"  # Azul más claro\n",
    "COLOR_BORDES = \"#BDBDBD\"  # Gris claro\n",
    "\n",
    "# ==================================================\n",
    "# Configuración del Chatbot\n",
    "# ==================================================\n",
    "\n",
    "class ChatbotApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        # Frame del chatbot\n",
    "        self.chatbot_frame = tk.LabelFrame(window, text=\"Chatbot de Alzheimer\", font = (\"Times New Roman\", 25), bg=COLOR_FONDO_PRINCIPAL,\n",
    "            fg=COLOR_TEXTO_PRINCIPAL,\n",
    "            relief=tk.RAISED,\n",
    "            borderwidth=2\n",
    "        )\n",
    "        self.chatbot_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "        # Scrollbar\n",
    "        self.scrollbar = Scrollbar(self.chatbot_frame)\n",
    "        self.chatbox = Text(\n",
    "            self.chatbot_frame,\n",
    "            height=15,\n",
    "            width=60,\n",
    "            yscrollcommand=self.scrollbar.set,\n",
    "            font = (\"Times New Roman\", 20),\n",
    "            bg=\"white\",\n",
    "            fg=COLOR_TEXTO_PRINCIPAL,\n",
    "            relief=tk.SUNKEN,\n",
    "            borderwidth=2\n",
    "            )\n",
    "        self.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.chatbox.pack(side = tk.TOP, fill = tk.BOTH, expand = True)\n",
    "\n",
    "### Deshabilitar la edición ###\n",
    "        self.chatbox.config(state = \"disabled\")\n",
    "\n",
    "        ### Configurar etiquetas (tags) para alinear los mensajes ###\n",
    "        self.chatbox.tag_configure(\"user\",justify = \"right\", background= \"#DCF8C6\", relief = \"raised\", borderwidth = 2)\n",
    "        self.chatbox.tag_configure(\"bot\", justify=\"left\", background=\"#ECECEC\", relief=\"raised\", borderwidth=2)\n",
    "\n",
    "        # Campo de entrada y botón de envío\n",
    "        self.entry_frame = tk.Frame(self.chatbot_frame, bg=COLOR_FONDO_PRINCIPAL)\n",
    "        self.entry_frame.pack(side = tk.BOTTOM, fill = tk.X, padx = 5, pady = 5)\n",
    "        \n",
    "        self.indicativo_label = tk.Label(\n",
    "            self.entry_frame,\n",
    "            text = \"Escribe tu mensaje aquí: \",\n",
    "            font = (\"Times New Roman\", 16),\n",
    "            bg=COLOR_FONDO_PRINCIPAL,\n",
    "            fg=COLOR_TEXTO_PRINCIPAL\n",
    "            )\n",
    "        self.indicativo_label.pack(side = tk.TOP, anchor = tk.W, pady = (0, 5))\n",
    "\n",
    "        ### Frame para el campo de entrada y el botón (dentro del entry_frame)###\n",
    "        self.input_frame = tk.Frame(self.entry_frame, bg=COLOR_FONDO_PRINCIPAL)\n",
    "        self.input_frame.pack(side = tk.TOP, fill = tk.X)\n",
    "\n",
    "        self.entry = tk.Entry(\n",
    "            self.input_frame,\n",
    "            width=50,\n",
    "            font = (\"Times New Roman\", 20),\n",
    "            bg = \"white\",\n",
    "            fg = COLOR_TEXTO_PRINCIPAL,\n",
    "            relief = tk.SUNKEN,\n",
    "            borderwidth = 2\n",
    "            )\n",
    "        \n",
    "        self.entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.send_button = tk.Button(\n",
    "            self.input_frame,\n",
    "            text=\"Enviar\",\n",
    "            command=self.send_message,\n",
    "            font = (\"Times New Roman\", 16)\n",
    "            )\n",
    "        self.send_button.pack(side = tk.RIGHT)\n",
    "\n",
    "        # Enviar mensaje al presionar Enter\n",
    "        self.entry.bind(\"<Return>\", lambda event: self.send_message())\n",
    "\n",
    "    def send_message(self):\n",
    "        user_text = self.entry.get().strip()\n",
    "        if user_text:\n",
    "\n",
    "            ### Habilitar temporalmente para insertar texto ###\n",
    "            self.chatbox.config(state = \"normal\" )\n",
    "\n",
    "            ### Mostrar la pregunta en el chat (Alineado a la derecha)###\n",
    "            self.chatbox.insert(tk.END, f\"Tú: {user_text}\\n\", \"user\")\n",
    "\n",
    "            ### Desplazar al final del chat###\n",
    "            self.chatbox.see(tk.END)\n",
    "\n",
    "            ### Enviar la pregunta a la API ###\n",
    "            response_text = self.get_response_from_api(user_text)\n",
    "\n",
    "            ### Mostrar la respuesta en el chat (Alineado a la izquierda)###\n",
    "            self.chatbox.insert(tk.END, f\"Bot: {response_text}\\n\\n\", \"bot\")\n",
    "\n",
    "            ### Desplazar al final del chat ###\n",
    "            self.chatbox.see(tk.END)  \n",
    "\n",
    "            ### Deshabilitar de nuevo ###\n",
    "            self.chatbox.config(state = \"disabled\")\n",
    "\n",
    "            ### Limpiar el campo de entrada ###\n",
    "            self.entry.delete(0, tk.END)\n",
    "\n",
    "    def get_response_from_api(self, question):\n",
    "        ### Lista de saludos comunes ###\n",
    "        saludos = [\"hola\", \"holis\", \"buenos días\", \"buenas tardes\", \"buenas noches\", \"hi\", \"hello\", \"que tal\", \"cómo estás\"]\n",
    "\n",
    "        ### Convertir la pregunta a minúsculas para hacer la comparación insensible a mayúsculas ###\n",
    "        question_lower = question.lower()\n",
    "\n",
    "        ### Verificar si la pregunta contiene un saludo (incluso con errores tipográficos) ###\n",
    "        contiene_saludo = any(fuzz.partial_ratio(saludo, question_lower) > 80 for saludo in saludos)\n",
    "        \n",
    "        ### Si contiene un saludo, responder con un saludo amigable ###\n",
    "        if contiene_saludo:\n",
    "            respuesta_saludo = \"¡Hola! Soy un chatbot diseñado para ayudarte con información sobre el Alzheimer. ¿En qué puedo ayudarte hoy?\"\n",
    "\n",
    "            ### Si la pregunta contiene algo más aparte del saludo, responder también a eso ###\n",
    "            if len(question_lower.split()) > 2: ### Si hay más de dos palabras, asumimos que hay una pregunta###\n",
    "\n",
    "        ### Si no es un saludo, enviar la pregunta a la API###\n",
    "\n",
    "                try:\n",
    "                    payload = {\"question\": question}\n",
    "                    response = requests.get(API_URL, json = payload)\n",
    "\n",
    "            # Verificar si la respuesta es un JSON válido\n",
    "                    try:\n",
    "                        response_json = response.json()  # Intenta convertir la respuesta a JSON\n",
    "                        \n",
    "                        if isinstance(response_json, dict):  # Si es un diccionario\n",
    "\n",
    "                            respuesta_api = response_json.get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "\n",
    "                        elif isinstance(response_json, list) and len(response_json) > 0:  # Si es una lista\n",
    "                            respuesta_api = response_json[0].get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "                        \n",
    "                        else:\n",
    "                            # Si la respuesta no es un diccionario ni una lista, devolver el texto directamente\n",
    "                            respuesta_api = str(response_json)\n",
    "\n",
    "                    except ValueError:  # Si no se puede convertir a JSON\n",
    "\n",
    "                        # Si la respuesta no es un JSON válido, devolver el texto directamente\n",
    "                        respuesta_api = response.text\n",
    "\n",
    "                except requests.RequestException as e:\n",
    "                    return f\"Error: No se pudo conectar con el servidor. Detalles: {str(e)}\"\n",
    "            \n",
    "            else:\n",
    "            # Si solo es un saludo, devolver solo el saludo\n",
    "                return respuesta_saludo\n",
    "\n",
    "    # Si no es un saludo, enviar la pregunta a la API\n",
    "        try:\n",
    "            payload = {\"question\": question}\n",
    "            response = requests.get(API_URL, json=payload)\n",
    "\n",
    "            # Verificar si la respuesta es un JSON válido\n",
    "            try:\n",
    "                response_json = response.json()  # Intenta convertir la respuesta a JSON\n",
    "                if isinstance(response_json, dict):  # Si es un diccionario\n",
    "                    return response_json.get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "                elif isinstance(response_json, list) and len(response_json) > 0:  # Si es una lista\n",
    "                    return response_json[0].get(\"response\", \"Lo siento, no pude obtener una respuesta.\")\n",
    "                else:\n",
    "                    # Si la respuesta no es un diccionario ni una lista, devolver el texto directamente\n",
    "                    return str(response_json)\n",
    "            except ValueError:  # Si no se puede convertir a JSON\n",
    "\n",
    "    # Si la respuesta no es un JSON válido, devolver el texto directamente\n",
    "                return response.text\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Error: No se pudo conectar con el servidor. Detalles: {str(e)}\"\n",
    "\n",
    "# ==================================================\n",
    "# Configuración del Monitoreo\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "def calcular_velocidad_habla(texto, tiempo_transcurrido):\n",
    "    palabras = texto.split()\n",
    "    if tiempo_transcurrido > 0:\n",
    "        velocidad = len(palabras) / tiempo_transcurrido  # Palabras por segundo\n",
    "        return velocidad\n",
    "    return 0\n",
    "\n",
    "\n",
    "def filtrar_ruido(audio_path, output_path=\"audio_filtrado.wav\"):\n",
    "    # Verificar si el archivo existe\n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"El archivo {audio_path} no existe. Esperando a que se cree...\")\n",
    "        return None  # O puedes retornar un valor por defecto o manejar el error de otra manera\n",
    "\n",
    "    # Cargar el audio\n",
    "    audio, rate = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Reducir el ruido\n",
    "    audio_filtrado = nr.reduce_noise(y=audio, sr=rate)\n",
    "\n",
    "    # Guardar el audio filtrado\n",
    "    sf.write(output_path, audio_filtrado, rate)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def detectar_pausas_largas(audio_path, umbral_silencio=-50, duracion_minima_pausa=3000):\n",
    "    # Cargar el archivo de audio\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Detectar silencios\n",
    "    silencios = detect_silence(audio, min_silence_len=duracion_minima_pausa, silence_thresh=umbral_silencio)\n",
    "\n",
    "    # Calcular la duración total de las pausas\n",
    "    duracion_total_pausas = sum(fin - inicio for inicio, fin in silencios)\n",
    "    return duracion_total_pausas / 1000  # Convertir a segundos\n",
    "\n",
    "\n",
    "\n",
    "def detectar_retraso_en_habla(texto, tiempo_transcurrido, audio_path):\n",
    "    if not texto:  # Si no hay texto, no hay alerta\n",
    "        return None\n",
    "\n",
    "    # Calcular velocidad del habla\n",
    "    velocidad = calcular_velocidad_habla(texto, tiempo_transcurrido)\n",
    "    \n",
    "    # Detectar pausas largas\n",
    "    duracion_pausas = detectar_pausas_largas(audio_path, umbral_silencio=-45)  # Ajustar umbral de silencio\n",
    "    \n",
    "    # Umbrales ajustados\n",
    "    umbral_velocidad = 0.6  # Menos de 0.6 palabras por segundo indica posible retraso\n",
    "    umbral_pausas = max(3.5, tiempo_transcurrido * 0.3)  # Pausas largas basadas en duración del audio\n",
    "    \n",
    "    # Calcular variabilidad en la velocidad del habla\n",
    "    palabras = texto.split()\n",
    "    if len(palabras) > 5:  # Solo calcular si hay suficientes palabras\n",
    "        tiempos_entre_palabras = [tiempo_transcurrido / len(palabras)] * len(palabras)  # Simulación básica\n",
    "        variabilidad = np.std(tiempos_entre_palabras)\n",
    "    else:\n",
    "        variabilidad = 0\n",
    "\n",
    "    umbral_variabilidad = 0.4  # Si la variabilidad es alta, hay inconsistencias en el habla\n",
    "    \n",
    "    alertas = []\n",
    "    if velocidad < umbral_velocidad:\n",
    "        alertas.append(f\"Velocidad del habla muy lenta ({velocidad:.2f} palabras/segundo).\")\n",
    "    if duracion_pausas > umbral_pausas:\n",
    "        alertas.append(f\"Pausas largas detectadas ({duracion_pausas:.1f} segundos).\")\n",
    "    if variabilidad > umbral_variabilidad:\n",
    "        alertas.append(f\"Patrón de habla inconsistente detectado (variabilidad: {variabilidad:.2f}).\")\n",
    "    \n",
    "    if alertas:\n",
    "        return \"Alerta: Posible retraso en el habla. \" + \" \".join(alertas)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def analizar_fluidez(texto, tiempo_transcurrido):\n",
    "    palabras = texto.split()\n",
    "    if len(palabras) < 3:  # Frase muy corta\n",
    "        return \"Alerta: Frase demasiado corta. Posible dificultad para hablar.\"\n",
    "\n",
    "    # Detectar repeticiones de palabras exactas o similares\n",
    "    repeticiones = {}\n",
    "    for i in range(len(palabras)):\n",
    "        palabra_actual = palabras[i]\n",
    "        if palabra_actual in repeticiones:\n",
    "            repeticiones[palabra_actual] += 1\n",
    "        else:\n",
    "            # Verificar si hay palabras similares ya registradas\n",
    "            for palabra_registrada in repeticiones:\n",
    "                if fuzz.ratio(palabra_actual, palabra_registrada) > 80:  # Umbral de similitud\n",
    "                    repeticiones[palabra_registrada] += 1\n",
    "                    break\n",
    "            else:\n",
    "                repeticiones[palabra_actual] = 1\n",
    "\n",
    "    for palabra, count in repeticiones.items():\n",
    "        if count > 3:  # Palabra repetida más de 3 veces\n",
    "            return f\"Alerta: Palabra '{palabra}' repetida {count} veces. Posible dificultad para hablar.\"\n",
    "\n",
    "    return None\n",
    "\n",
    "cap = None\n",
    "camara_encendida = False\n",
    "\n",
    "class EnhancedCameraApp:\n",
    "    def __init__(self, window, window_label, alertas_text):\n",
    "        self.window = window\n",
    "        self.window_label = window_label\n",
    "        self.alertas_text = alertas_text\n",
    "        self.video_source = 0\n",
    "        self.vid = None\n",
    "        self.is_camera_on = False\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.muted = False\n",
    "        self.tts_engine = pyttsx3.init()\n",
    "        self.voices = self.tts_engine.getProperty('voices')\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone()\n",
    "        self.is_listening = False\n",
    "        self.setup_ui()\n",
    "\n",
    "\n",
    "    def on_close(self):\n",
    "        # Detener el reconocimiento de voz y la cámara al cerrar la ventana\n",
    "        self.stop_listening()\n",
    "        self.stop_camera()\n",
    "\n",
    "    def stop_listening(self):\n",
    "        if self.is_listening:\n",
    "            self.is_listening = False  # Detener el hilo de reconocimiento de voz\n",
    "            self.btn_listen.config(text=\"Iniciar Reconocimiento\")\n",
    "\n",
    "        # Liberar el micrófono\n",
    "            if hasattr(self, 'microphone'):\n",
    "                self.microphone.__exit__(None, None, None)\n",
    "\n",
    "    def setup_ui(self):\n",
    "        control_frame = ttk.Frame(self.window, style = \"Custom.TFrame\")\n",
    "        control_frame.pack(pady=10)\n",
    "\n",
    "        self.btn_frame = ttk.Frame(self.window, style = \"Custom.TFrame\")\n",
    "        self.btn_frame.pack(pady=10)\n",
    "\n",
    "        self.btn_start = tk.Button(\n",
    "            self.btn_frame,\n",
    "            text=\"Iniciar Cámara\",\n",
    "            command=self.start_camera,\n",
    "            font = (\"Times New Roman\", 16)\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.btn_start.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_stop = tk.Button(\n",
    "            self.btn_frame,\n",
    "            text=\"Detener Cámara\",\n",
    "            command=self.stop_camera,\n",
    "            state=tk.DISABLED,\n",
    "            font = (\"Times New Roman\", 16)\n",
    "            )\n",
    "        \n",
    "        self.btn_stop.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_mute = tk.Button(\n",
    "            self.btn_frame,\n",
    "            text=\"Silenciar\",\n",
    "            command=self.toggle_mute,\n",
    "            font = (\"Times New Roman\", 16)\n",
    "            )\n",
    "        self.btn_mute.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.btn_listen = tk.Button(\n",
    "            self.btn_frame,\n",
    "            text=\"Iniciar Reconocimiento\",\n",
    "            command=self.toggle_listen,\n",
    "            font = (\"Times New Roman\", 16)\n",
    "            )\n",
    "        self.btn_listen.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.canvas = tk.Canvas(self.window, width=640, height=480, bg = COLOR_FONDO_PRINCIPAL)\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def start_camera(self):\n",
    "        global cap, camara_encendida, monitoreo_activo  # Acceder a las variables globales\n",
    "        if not self.is_camera_on:\n",
    "            self.vid = cv2.VideoCapture(self.video_source)\n",
    "            if not self.vid.isOpened():\n",
    "                print(\"Error: No se pudo abrir la cámara.\")\n",
    "                return\n",
    "            self.is_camera_on = True\n",
    "            cap = self.vid  # Actualizar la variable global `cap`\n",
    "            camara_encendida = True  # Actualizar el estado de la cámara\n",
    "            activar_monitoreo(True)  # Activar el monitoreo\n",
    "            self.update_camera()\n",
    "            self.btn_start.config(state=tk.DISABLED)\n",
    "            self.btn_stop.config(state=tk.NORMAL)\n",
    "\n",
    "# Modificar la función stop_camera para desactivar el monitoreo\n",
    "    def stop_camera(self):\n",
    "        global cap, camara_encendida, monitoreo_activo  # Acceder a las variables globales\n",
    "        if self.is_camera_on:\n",
    "            self.is_camera_on = False\n",
    "            self.vid.release()\n",
    "            cap = None  # Limpiar la variable global `cap`\n",
    "            camara_encendida = False  # Actualizar el estado de la cámara\n",
    "            activar_monitoreo(False)  # Desactivar el monitoreo\n",
    "            self.canvas.delete(\"all\")\n",
    "            self.btn_start.config(state=tk.NORMAL)\n",
    "            self.btn_stop.config(state=tk.DISABLED)\n",
    "\n",
    "    def toggle_mute(self):\n",
    "        self.muted = not self.muted\n",
    "        self.btn_mute.config(text=\"Activar sonido\" if self.muted else \"Silenciar\")\n",
    "\n",
    "    def update_camera(self):\n",
    "        if self.is_camera_on:\n",
    "            ret, frame = self.vid.read()\n",
    "            if ret:\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                img = Image.fromarray(rgb)\n",
    "                imgtk = ImageTk.PhotoImage(image=img)\n",
    "                self.canvas.imgtk = imgtk\n",
    "                self.canvas.create_image(0, 0, anchor=tk.NW, image=imgtk)\n",
    "            self.window.after(30, self.update_camera)\n",
    "\n",
    "    def toggle_listen(self):\n",
    "        global reconocimiento_voz_activo\n",
    "        if not self.is_listening:\n",
    "            self.is_listening = True\n",
    "            reconocimiento_voz_activo = True  # Activar el reconocimiento de voz\n",
    "            activar_monitoreo(True)  # Activar el monitoreo\n",
    "            self.btn_listen.config(text=\"Detener Reconocimiento\")\n",
    "            threading.Thread(target=self.recognize_speech, daemon=True).start()\n",
    "        else:\n",
    "            self.is_listening = False\n",
    "            reconocimiento_voz_activo = False  # Desactivar el reconocimiento de voz\n",
    "            activar_monitoreo(False)  # Desactivar el monitoreo\n",
    "            self.btn_listen.config(text=\"Iniciar Reconocimiento\")\n",
    "    def recognize_speech(self):\n",
    "        try:\n",
    "            with self.microphone as source:\n",
    "                self.recognizer.adjust_for_ambient_noise(source, duration=2)\n",
    "                if app_running:\n",
    "                    self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Sistema listo para escuchar...\\n\")\n",
    "                    self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "                while self.is_listening and app_running:\n",
    "                    try:\n",
    "                        inicio = time.time()\n",
    "                        if app_running:\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Escuchando...\\n\")\n",
    "                            self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "                    # Escuchar el audio desde el micrófono\n",
    "                        audio = self.recognizer.listen(source, timeout=5)\n",
    "                        fin = time.time()\n",
    "                        tiempo_transcurrido = fin - inicio\n",
    "\n",
    "                    # Guardar el audio en un archivo temporal\n",
    "                        with open(\"temp_audio.wav\", \"wb\") as f:\n",
    "                            f.write(audio.get_wav_data())\n",
    "\n",
    "                    # Filtrar el ruido del audio\n",
    "                        audio_filtrado_path = filtrar_ruido(\"temp_audio.wav\")\n",
    "\n",
    "                    # Transcribir el audio filtrado\n",
    "                        texto = self.recognizer.recognize_google(audio_filtrado_path, language=\"es-ES\")\n",
    "                        if app_running:\n",
    "                            self.window.after(0, self.display_recognized_text, texto)\n",
    "\n",
    "                    # Analizar fluidez y coherencia\n",
    "                        alerta_fluidez = analizar_fluidez(texto, tiempo_transcurrido)\n",
    "                        if alerta_fluidez and app_running:\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] {alerta_fluidez}\\n\")\n",
    "                            self.alertas_text.see(tk.END)\n",
    "\n",
    "                        alerta_retraso_habla = detectar_retraso_en_habla(texto, tiempo_transcurrido, audio)\n",
    "                        if alerta_retraso_habla and app_running:\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] {alerta_retraso_habla}\\n\")\n",
    "                            self.alertas_text.see(tk.END)\n",
    "\n",
    "                    # Detectar pausas largas\n",
    "                        duracion_pausas = detectar_pausas_largas(audio_filtrado_path)\n",
    "                        if duracion_pausas > 4.0:  # Umbral de pausas largas\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Alerta: Pausas largas detectadas ({duracion_pausas:.1f} segundos).\\n\")\n",
    "                            self.alertas_text.see(tk.END)\n",
    "\n",
    "                    except sr.UnknownValueError:\n",
    "                        if app_running:\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] No se pudo entender el audio. Por favor, repite.\\n\")\n",
    "                            self.window.after(0, self.alertas_text.see, tk.END)\n",
    "                    except sr.RequestError as e:\n",
    "                        if app_running:\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Error en la solicitud al servicio de reconocimiento: {e}\\n\")\n",
    "                            self.window.after(0, self.alertas_text.see, tk.END)\n",
    "                    except sr.WaitTimeoutError:\n",
    "                        if app_running:\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Tiempo de espera agotado. Intenta de nuevo.\\n\")\n",
    "                            self.window.after(0, self.alertas_text.see, tk.END)\n",
    "                    except Exception as e:\n",
    "                        if app_running:\n",
    "                            self.window.after(0, self.alertas_text.insert, tk.END, f\"[{datetime.now()}] Error inesperado: {e}\\n\")\n",
    "                            self.window.after(0, self.alertas_text.see, tk.END)\n",
    "        except Exception as e:\n",
    "            print(f\"Error en recognize_speech: {e}\")\n",
    "            self.window.after(0, self.alertas_text.see, tk.END)\n",
    "\n",
    "\n",
    "    def display_recognized_text(self, text):\n",
    "        self.alertas_text.insert(tk.END, f\"[Voz Detectada] {text}\\n\")\n",
    "        self.alertas_text.see(tk.END)\n",
    "\n",
    "# Función para detectar actividad física\n",
    "prev_frame = None\n",
    "\n",
    "def detectar_personas(frame):\n",
    "    \"\"\"Detecta si hay personas en la imagen usando YOLOv4\"\"\"\n",
    "    height, width = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5 and classes[class_id] == \"person\":\n",
    "                return True  # Se detectó una persona\n",
    "\n",
    "    return False  # No se detectó ninguna persona  # No se detectó ninguna persona\n",
    "\n",
    "\n",
    "def detectar_movimiento():\n",
    "    \"\"\"Detecta movimiento solo si hay una persona en la imagen\"\"\"\n",
    "    global prev_frame, cap\n",
    "    if not camara_encendida or cap is None:\n",
    "        return False\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return False\n",
    "\n",
    "    # Verificar si hay personas en la imagen antes de analizar movimiento\n",
    "    if not detectar_personas(frame):\n",
    "        return False  # No hay personas, no analizar movimiento\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "\n",
    "    if prev_frame is None:\n",
    "        prev_frame = gray\n",
    "        return True  # Si es el primer frame, considerar que no hay movimiento\n",
    "\n",
    "    frame_delta = cv2.absdiff(prev_frame, gray)\n",
    "    prev_frame = gray\n",
    "    movimiento = np.sum(frame_delta) > 500000  # Umbral de cambio en píxeles\n",
    "\n",
    "    return not movimiento\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detectar_emocion():\n",
    "    global cap\n",
    "    if not camara_encendida or cap is None:\n",
    "        return \"No se detecta cámara\"\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return \"No se pudo capturar imagen\"\n",
    "    \n",
    "    try:\n",
    "        # Redimensionar la imagen al tamaño esperado por el modelo (224x224)\n",
    "        resized = cv2.resize(frame, (224, 224))\n",
    "        \n",
    "        # Convertir la imagen a un array y normalizarla\n",
    "        resized = resized / 255.0\n",
    "        \n",
    "        # Expandir las dimensiones para que coincidan con la entrada del modelo\n",
    "        resized = np.expand_dims(resized, axis=0)\n",
    "        \n",
    "        # Predecir la emoción\n",
    "        predicciones = emociones_model.predict(resized)\n",
    "        \n",
    "        # Imprimir las predicciones para depuración\n",
    "        print(\"Predicciones:\", predicciones)\n",
    "        \n",
    "        # Obtener la emoción predicha (índice con la probabilidad más alta)\n",
    "        emocion_predicha = np.argmax(predicciones, axis=1)[0]\n",
    "        emociones = [\"Enojo\", \"Feliz\", \"Tristeza\", \"Sorpresa\", \"Neutral\"]\n",
    "        if emocion_predicha >= len(emociones):\n",
    "            return f\"Error: Índice de emoción fuera de rango ({emocion_predicha})\"\n",
    "        \n",
    "        # Obtener la emoción detectada\n",
    "        emocion_detectada = emociones[emocion_predicha]\n",
    "        \n",
    "        return f\"Emoción detectada: {emocion_detectada}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error al detectar emoción: {str(e)}\"\n",
    "\n",
    "# Configuración de reconocimiento de voz\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detectar_cambios_habla():\n",
    "    try:\n",
    "        with sr.Microphone() as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            try:\n",
    "                inicio = time.time()  # Registrar el tiempo de inicio\n",
    "                audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)\n",
    "                fin = time.time()  # Registrar el tiempo de finalización\n",
    "                tiempo_transcurrido = fin - inicio  # Calcular el tiempo transcurrido\n",
    "\n",
    "                # Guardar el audio en un archivo temporal\n",
    "                with open(\"temp_audio.wav\", \"wb\") as f:\n",
    "                    f.write(audio.get_wav_data())\n",
    "\n",
    "                # Transcribir el audio\n",
    "                texto = recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "                \n",
    "                # Analizar fluidez y coherencia del habla\n",
    "                alerta_fluidez = analizar_fluidez(texto, tiempo_transcurrido)  # Pasar tiempo_transcurrido\n",
    "                if alerta_fluidez:\n",
    "                    return alerta_fluidez\n",
    "                \n",
    "                # Detectar retraso en el habla\n",
    "                alerta_retraso_habla = detectar_retraso_en_habla(texto, tiempo_transcurrido, \"temp_audio.wav\")\n",
    "                if alerta_retraso_habla:\n",
    "                    return alerta_retraso_habla\n",
    "                \n",
    "                return texto\n",
    "            except sr.WaitTimeoutError:\n",
    "                return \"Alerta: No se detectó sonido en el tiempo esperado.\"\n",
    "            except sr.UnknownValueError:\n",
    "                return \"Alerta: No se detectó habla.\"\n",
    "            except sr.RequestError:\n",
    "                return \"Alerta: Error en reconocimiento.\"\n",
    "    except OSError:\n",
    "        return \"Alerta: No se detecta micrófono, no se puede realizar monitoreo del habla.\"\n",
    "\n",
    "# Variable global para controlar el estado del monitoreo\n",
    "monitoreo_activo = False\n",
    "app_running = True\n",
    "reconocimiento_voz_activo = False\n",
    "tiempo_inicio_inactividad = None\n",
    "umbral_inactividad = 20\n",
    "# Función para activar/desactivar el monitoreo\n",
    "def activar_monitoreo(estado):\n",
    "    global monitoreo_activo\n",
    "    monitoreo_activo = estado\n",
    "    \n",
    "\n",
    "# Monitoreo en tiempo real\n",
    "def monitorear_paciente():\n",
    "    global app_running, camara_encendida, reconocimiento_voz_activo, tiempo_inicio_inactividad  # Asegurar acceso a la variable global\n",
    "\n",
    "    while app_running:  # Solo monitorear si la aplicación está en ejecución\n",
    "        if monitoreo_activo and camara_encendida:  # Solo monitorear si la cámara está encendida\n",
    "            # Verificar si hay personas en el rango de visión de la cámara\n",
    "            ret, frame = cap.read() if cap else (False, None)\n",
    "            hay_personas = detectar_personas(frame) if ret else False\n",
    "\n",
    "            if not hay_personas:\n",
    "                # Si no hay personas, emitir alerta de \"No se detecta ninguna persona\"\n",
    "                alerta = \"Alerta: No se detecta ninguna persona.\"\n",
    "                if app_running:\n",
    "                    alertas_text.config(state=tk.NORMAL)  # Habilitar la edición temporalmente\n",
    "                    alertas_text.insert(tk.END, f\"[{datetime.now()}] {alerta}\\n\")\n",
    "                    alertas_text.see(tk.END)\n",
    "                    alertas_text.config(state=tk.DISABLED)  # Deshabilitar la edición\n",
    "            else:\n",
    "                # Si hay personas, verificar la actividad física\n",
    "                actividad_fisica = detectar_movimiento()\n",
    "                habla = \"\"\n",
    "                if reconocimiento_voz_activo:  # Solo monitorear el habla si el reconocimiento de voz está activo\n",
    "                    habla = detectar_cambios_habla()\n",
    "                emocion = detectar_emocion()\n",
    "                alertas = []\n",
    "\n",
    "                if actividad_fisica:  # Si no hay movimiento\n",
    "                    if tiempo_inicio_inactividad is None:\n",
    "                        tiempo_inicio_inactividad = time.time()  # Iniciar el temporizador\n",
    "                    else:\n",
    "                        tiempo_transcurrido = time.time() - tiempo_inicio_inactividad\n",
    "                        if tiempo_transcurrido >= umbral_inactividad:\n",
    "                            alertas.append(f\"Alerta: Inactividad prolongada detectada ({int(tiempo_transcurrido / 60)} minutos).\")\n",
    "                            tiempo_inicio_inactividad = None  # Reiniciar el temporizador\n",
    "                else:\n",
    "                    tiempo_inicio_inactividad = None  # Reiniciar el temporizador si hay movimiento\n",
    "\n",
    "                # Alertas existentes\n",
    "                if actividad_fisica and tiempo_inicio_inactividad is not None:\n",
    "                    alertas.append(\"Alerta: Baja actividad física detectada.\")\n",
    "                if reconocimiento_voz_activo and \"Alerta\" in habla:  # Verificar si hay una alerta en el habla\n",
    "                    alertas.append(habla)\n",
    "                if reconocimiento_voz_activo and \"Alerta\" in habla:  # Verificar si hay una alerta en el habla\n",
    "                    alertas.append(habla)\n",
    "\n",
    "                alertas.append(emocion)\n",
    "\n",
    "                if app_running:  # Verificar si la ventana todavía está abierta\n",
    "                    for alerta in alertas:\n",
    "                        alertas_text.config(state=tk.NORMAL)  # Habilitar la edición temporalmente\n",
    "                        alertas_text.insert(tk.END, f\"[{datetime.now()}] {alerta}\\n\")\n",
    "                        alertas_text.see(tk.END)\n",
    "                        alertas_text.config(state=tk.DISABLED)  # Deshabilitar la edición\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Función para predecir\n",
    "def seleccionar_imagen():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    title=\"Selecciona una imagen\",\n",
    "    filetypes=((\"Archivos de imagen\", \".jpg *.jpeg *.png\"), (\"Todos los archivos\", \".*\"))\n",
    "    return file_path\n",
    "\n",
    "def predecir_imagen(file_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(file_path, target_size=(150, 150))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediccion = model.predict(img)\n",
    "    clase_predicha = np.argmax(prediccion, axis=1)\n",
    "    return clase_predicha[0]\n",
    "\n",
    "def predecir_y_mostrar():\n",
    "    file_path = seleccionar_imagen()\n",
    "    if file_path:\n",
    "\n",
    "        ### Cargar Imágen Seleccionada ###\n",
    "\n",
    "        img = Image.open(file_path)\n",
    "\n",
    "        ### Redimensionar la imágen para que quepa en la interfaz ###\n",
    "        img = img.resize((300, 300), Image.Resampling.LANCZOS)\n",
    "\n",
    "        img_tk = ImageTk.PhotoImage(img)\n",
    "\n",
    "        #Mostrar la Imágen en el widget de resultado\n",
    "        imagen_label.config(image = img_tk)\n",
    "\n",
    "        ### Mantener una referencia para evitar que la imágen sea eliminada por el recolector de basura###\n",
    "        imagen_label.image = img_tk\n",
    "    \n",
    "    \n",
    "        if file_path:\n",
    "            clase_predicha = predecir_imagen(file_path)\n",
    "        if clase_predicha == 0:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que se encuentra en la etapa leve del alzheimer.\\n Se sugiere que opte por participar en actividades cognitivas y físicas para estimular el cerebro.\\n Consultar regularmente in médico especializado en neurología.\\n  Se observa una disminución en el volumen del tejido cerebral, especialmente en áreas como el hipocampo (crucial para la memoria) y la corteza cerebral. Esta atrofia es un indicador clave de la enfermedad de Alzheimer\"\n",
    "        elif clase_predicha == 1:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que se encuentra en la etapa avanzada del alzheimer. \\n Se observa una atrofia cerebral más pronunciada. \\n Los ventrículos están más agrandados y la corteza cerebral muestra una mayor pérdida de volumen.\\n Se recomienda evaluaciones médicas de forma frecuente\"\n",
    "        elif clase_predicha == 2:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que no posee alzheimer. \\n  En general, la estructura cerebral parece estar dentro de los límites normales para la edad de la persona. \\n No se observa una atrofia cerebral significativa. El volumen del tejido cerebral parece conservado.\"\n",
    "        elif clase_predicha == 3:\n",
    "            resultado = \"La imagen proporcionada pertenece a una persona que puede tener indicios de padecer alzheimer. \\n Se observa que el patrón de atrofia es consistente con la enfermedad de Alzheimer en una etapa muy temprana. Por lo tanto, existe una posibilidad de que esta persona padezca de alzheimer \"\n",
    "        else:\n",
    "            resultado = \"La imagen proporcionada es incorrecta.\"\n",
    "        \n",
    "        resultado_text.config(state = tk.NORMAL)\n",
    "        resultado_text.delete(1.0, tk.END)\n",
    "\n",
    "# Calcular el número de líneas vacías necesarias para centrar verticalmente\n",
    "        ### Mostrar el resultado en el widget de texto ###\n",
    "        resultado_text.config(state = tk.NORMAL)\n",
    "        resultado_text.delete(1.0, tk.END)  \n",
    "\n",
    "        # Calcular el número de líneas vacías necesarias para centrar verticalmente\n",
    "        num_lineas = resultado_text.winfo_height() // resultado_text.dlineinfo(\"@0,0\")[3]  # Altura en líneas\n",
    "        texto_lineas = resultado.count(\"\\n\") + 1  # Número de líneas del texto\n",
    "        lineas_vacias = (num_lineas - texto_lineas) // 2  # Líneas vacías antes del texto\n",
    "\n",
    "        # Insertar líneas vacías antes del texto\n",
    "        resultado_text.insert(tk.END, \"\\n\" * lineas_vacias)\n",
    "\n",
    "        resultado_text.insert(tk.END, resultado, \"centrado\")\n",
    "\n",
    "        # Insertar líneas vacías después del texto\n",
    "        resultado_text.insert(tk.END, \"\\n\" * lineas_vacias)\n",
    "\n",
    "        resultado_text.config(state=tk.DISABLED)\n",
    "\n",
    "# ==================================================\n",
    "# Interfaz gráfica\n",
    "# ==================================================\n",
    "\n",
    "ventana = tk.Tk()\n",
    "ventana.title(\"Memory Vision\")\n",
    "ventana.geometry(\"800x600\")\n",
    "ventana.configure(bg=COLOR_FONDO_PRINCIPAL)\n",
    "\n",
    "\n",
    "def on_close():\n",
    "    global app_running, reconocimiento_voz_activo\n",
    "\n",
    "    # Detener el hilo de monitoreo\n",
    "    app_running = False\n",
    "\n",
    "    # Detener el reconocimiento de voz\n",
    "    if reconocimiento_voz_activo:\n",
    "        camera_app.stop_listening()  # Detener el reconocimiento de voz\n",
    "        reconocimiento_voz_activo = False\n",
    "\n",
    "    # Detener la cámara\n",
    "    camera_app.stop_camera()  # Detener la cámara\n",
    "\n",
    "    # Cerrar la ventana\n",
    "    ventana.destroy()\n",
    "\n",
    "\n",
    "# Vincular el cierre de la ventana a la función on_close\n",
    "ventana.protocol(\"WM_DELETE_WINDOW\", on_close)\n",
    "\n",
    "\n",
    "style = ttk.Style()\n",
    "\n",
    "style.configure(\"Custom.TFrame\", background=COLOR_FONDO_PRINCIPAL)\n",
    "\n",
    "style.configure(\"Large.TButton\",\n",
    "                font = (\"Times New Roman\", 16)\n",
    "                )\n",
    "\n",
    "style.configure(\"TEntry\", font = (\"Times New Roman\", 16), background = \"white\", foreground = COLOR_TEXTO_PRINCIPAL)\n",
    "\n",
    "style.configure(\"TText\", font = (\"Times New Roman\", 16), background = \"white\", foreground = COLOR_TEXTO_PRINCIPAL)\n",
    "\n",
    "style.configure(\"TLabel\", font = (\"Times New Roman\", 16),  background = COLOR_FONDO_PRINCIPAL, foreground = COLOR_TEXTO_PRINCIPAL)\n",
    "\n",
    "notebook = ttk.Notebook(ventana)\n",
    "notebook.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "### Pestaña Monitoreo ###\n",
    "monitoreo_frame = ttk.Frame(notebook, style = \"Custom.TFrame\")\n",
    "notebook.add(monitoreo_frame, text=\"Monitoreo\")\n",
    "\n",
    "alertas_frame = tk.LabelFrame(\n",
    "    monitoreo_frame,\n",
    "    text=\"Alertas\",\n",
    "    font = (\"Times New Roman\", 25)\n",
    "    )\n",
    "alertas_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "alertas_text = tk.Text(\n",
    "    alertas_frame,\n",
    "    height=7,\n",
    "    width=40,\n",
    "    font = (\"Times New Roman\", 20)\n",
    "    )\n",
    "alertas_text.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "alertas_text.config(state = \"disabled\")\n",
    "\n",
    "camara_frame = ttk.Frame(monitoreo_frame)\n",
    "camara_frame.pack(pady=5)\n",
    "\n",
    "camara_label = ttk.Label(camara_frame)\n",
    "camara_label.pack()\n",
    "\n",
    "camera_app = EnhancedCameraApp(camara_frame, camara_label, alertas_text)\n",
    "\n",
    "### Pestaña Chatbot ###\n",
    "chatbot_frame = tk.Frame(notebook)\n",
    "notebook.add(chatbot_frame, text=\"Chatbot\")\n",
    "\n",
    "chatbot_system = ChatbotApp(chatbot_frame)\n",
    "\n",
    "### Pestaña Predicción ###\n",
    "prediccion_frame = ttk.Frame(notebook)\n",
    "notebook.add(prediccion_frame, text=\"Predicción\")\n",
    "\n",
    "pred_frame = tk.LabelFrame(prediccion_frame, text=\"Predicción de la imagen\", font = (\"Times New Roman\", 25))\n",
    "pred_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "predecir_button = tk.Button(pred_frame, text=\"Seleccionar Imagen y Predecir\", command=predecir_y_mostrar, font = (\"Times New Roman\", 16))\n",
    "predecir_button.pack(pady=5)\n",
    "\n",
    "imagen_label = tk.Label(pred_frame)\n",
    "imagen_label.pack(pady = 10)\n",
    "\n",
    "resultado_text = tk.Text(pred_frame, height=25, width=200, font = (\"Times New Roman\", 20))\n",
    "resultado_text.pack(padx=10, pady=70)\n",
    "\n",
    "resultado_text.tag_configure(\"centrado\", justify=\"center\")\n",
    "\n",
    "### Habilitar edición temporalmente ###\n",
    "resultado_text.config(state=\"normal\")\n",
    "\n",
    "### Volver a deshabilitar ###\n",
    "resultado_text.config(state=\"disabled\")\n",
    "\n",
    "# Iniciar monitoreo en segundo plano\n",
    "monitoreo_thread = threading.Thread(target=monitorear_paciente, daemon=True)\n",
    "monitoreo_thread.start()\n",
    "ventana.protocol(\"WM_DELETE_WINDOW\", on_close)\n",
    "ventana.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
